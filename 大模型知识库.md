## 一、模型结构

### 1、语言类：

#### a、GPT

#### b、Llama

### 2、多模态







## 二、训练/微调

```shell
# 参考链接：
1、https://zhuanlan.zhihu.com/p/682082440
```



### 1、全参数微调

### 2、高效参数微调

#### 2.1、增加额外参数 Addition-Based

##### 2.1.1 Prefix Tuning

```shell
# 固定LM，为LM添加可训练，任务特定的前缀，Prefix是可以学习的“隐式”的提示
```

##### 2.1.2 Prompt Tuning

```shell
# Prompt是人为构造的“显式”的提示，无法更新参数，可看作Prefix的简化版本
```

##### 2.1.3 P-Tuning



##### 2.1.4 P-Tuning V2

```shell
#  每一层都加入prompts tokens（重点学习）
1、移除冲参数化的编码器
2、针对不同任务采用不同的提示长度
3、引入多任务学习
4、回归传统的分类标签范式，而不是映射器
```

##### 2.1.5 Adapter Tuning

```shell
# 针对每一个Transformer层，增加了两个Adapter结构，分别是多头注意力的投影之后和第二个feed-forward层之后，只对新增的 Adapter 结构和 Layer Norm 层进行微调。
```

##### 2.1.6 AdapterFusion



##### 2.1.7 AdapterDrop

```shell
# 通过从较低的 Transformer 层删除可变数量的Adaper来提升推理速度。
```

##### 2.1.8 Instruction Tuning

```shell
# SFT的一种特殊形式
```



#### 2.2、选取一部分参数更新

```shell
# 只更新bias参数和特定任务的分类层参数
BitFit
```



#### 2.3、引入重参数化 Reparametrization-Based

##### 2.3.1 Lora

```shell
# LoRA 与 Transformer 的结合也很简单, 仅在 QKV Attention 的计算中增加一个旁路。
```



##### 2.3.2 AdaLora

```shell
# AdaLoRA是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。
1、调整增量矩分配。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。
2、以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法通过减少它们的参数预算来加速计算，同时，保留未来恢复的可能性并稳定训练。
3、在训练损失中添加了额外的惩罚项，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练。
```



##### 2.3.3 QLora

```shell
# QLoRA使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。
```



##### 2.3.4 MOELora



#### 2.4、混合高效微调

```shell
MAM Adapter、UniPELT
```



### 3、PEFT

```
https://github.com/huggingface/peft
```



## 三、prompt工程





## 四、相关开源项目

### 1、知识问答

### 2、大模型工具

#### a、LangChain

#### b、Deepspeed

#### c、BLIP2

#### d、Llama

### 3、大模型应用

### 4、智能助理

### 5、RAG技术

#### 6、文生图

## 五、其它

### 1、AIGC